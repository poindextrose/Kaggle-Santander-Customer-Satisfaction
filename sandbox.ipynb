{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_pickle_filename = \"train.dataframe.pickle\"\n",
    "test_pickle_filename = \"test.dataframe.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(train_pickle_filename):\n",
    "    df = pd.read_csv('train.csv', index_col='ID')\n",
    "    df.to_pickle(train_pickle_filename)\n",
    "else:\n",
    "    df = pd.read_pickle(train_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(test_pickle_filename):\n",
    "    df_test = pd.read_csv('test.csv', index_col='ID')\n",
    "    df_test.to_pickle(test_pickle_filename)\n",
    "else:\n",
    "    df_test = pd.read_pickle(test_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>imp_op_var40_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>7.602000e+04</td>\n",
       "      <td>76020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1523.199277</td>\n",
       "      <td>33.212865</td>\n",
       "      <td>86.208265</td>\n",
       "      <td>72.363067</td>\n",
       "      <td>119.529632</td>\n",
       "      <td>3.559130</td>\n",
       "      <td>6.472698</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>3.160715</td>\n",
       "      <td>...</td>\n",
       "      <td>7.935824</td>\n",
       "      <td>1.365146</td>\n",
       "      <td>12.215580</td>\n",
       "      <td>8.784074</td>\n",
       "      <td>31.505324</td>\n",
       "      <td>1.858575</td>\n",
       "      <td>76.026165</td>\n",
       "      <td>56.614351</td>\n",
       "      <td>1.172358e+05</td>\n",
       "      <td>0.039569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39033.462364</td>\n",
       "      <td>12.956486</td>\n",
       "      <td>1614.757313</td>\n",
       "      <td>339.315831</td>\n",
       "      <td>546.266294</td>\n",
       "      <td>93.155749</td>\n",
       "      <td>153.737066</td>\n",
       "      <td>30.604864</td>\n",
       "      <td>36.513513</td>\n",
       "      <td>95.268204</td>\n",
       "      <td>...</td>\n",
       "      <td>455.887218</td>\n",
       "      <td>113.959637</td>\n",
       "      <td>783.207399</td>\n",
       "      <td>538.439211</td>\n",
       "      <td>2013.125393</td>\n",
       "      <td>147.786584</td>\n",
       "      <td>4040.337842</td>\n",
       "      <td>2852.579397</td>\n",
       "      <td>1.826646e+05</td>\n",
       "      <td>0.194945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.163750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.787061e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.064092e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187563e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>238.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>12888.030000</td>\n",
       "      <td>21024.810000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>11073.570000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>...</td>\n",
       "      <td>50003.880000</td>\n",
       "      <td>20385.720000</td>\n",
       "      <td>138831.630000</td>\n",
       "      <td>91778.730000</td>\n",
       "      <td>438329.220000</td>\n",
       "      <td>24650.010000</td>\n",
       "      <td>681462.900000</td>\n",
       "      <td>397884.300000</td>\n",
       "      <td>2.203474e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                var3         var15  imp_ent_var16_ult1  \\\n",
       "count   76020.000000  76020.000000        76020.000000   \n",
       "mean    -1523.199277     33.212865           86.208265   \n",
       "std     39033.462364     12.956486         1614.757313   \n",
       "min   -999999.000000      5.000000            0.000000   \n",
       "25%         2.000000     23.000000            0.000000   \n",
       "50%         2.000000     28.000000            0.000000   \n",
       "75%         2.000000     40.000000            0.000000   \n",
       "max       238.000000    105.000000       210000.000000   \n",
       "\n",
       "       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 72.363067               119.529632   \n",
       "std                 339.315831               546.266294   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               12888.030000             21024.810000   \n",
       "\n",
       "       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  3.559130                 6.472698   \n",
       "std                  93.155749               153.737066   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max                8237.820000             11073.570000   \n",
       "\n",
       "       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  imp_op_var40_ult1  \\\n",
       "count             76020.000000             76020.000000       76020.000000   \n",
       "mean                  0.412946                 0.567352           3.160715   \n",
       "std                  30.604864                36.513513          95.268204   \n",
       "min                   0.000000                 0.000000           0.000000   \n",
       "25%                   0.000000                 0.000000           0.000000   \n",
       "50%                   0.000000                 0.000000           0.000000   \n",
       "75%                   0.000000                 0.000000           0.000000   \n",
       "max                6600.000000              6600.000000        8237.820000   \n",
       "\n",
       "           ...       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "count      ...                  76020.000000             76020.000000   \n",
       "mean       ...                      7.935824                 1.365146   \n",
       "std        ...                    455.887218               113.959637   \n",
       "min        ...                      0.000000                 0.000000   \n",
       "25%        ...                      0.000000                 0.000000   \n",
       "50%        ...                      0.000000                 0.000000   \n",
       "75%        ...                      0.000000                 0.000000   \n",
       "max        ...                  50003.880000             20385.720000   \n",
       "\n",
       "       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "count            76020.000000            76020.000000   \n",
       "mean                12.215580                8.784074   \n",
       "std                783.207399              538.439211   \n",
       "min                  0.000000                0.000000   \n",
       "25%                  0.000000                0.000000   \n",
       "50%                  0.000000                0.000000   \n",
       "75%                  0.000000                0.000000   \n",
       "max             138831.630000            91778.730000   \n",
       "\n",
       "       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 31.505324                 1.858575   \n",
       "std                2013.125393               147.786584   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max              438329.220000             24650.010000   \n",
       "\n",
       "       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\n",
       "count            76020.000000            76020.000000  7.602000e+04   \n",
       "mean                76.026165               56.614351  1.172358e+05   \n",
       "std               4040.337842             2852.579397  1.826646e+05   \n",
       "min                  0.000000                0.000000  5.163750e+03   \n",
       "25%                  0.000000                0.000000  6.787061e+04   \n",
       "50%                  0.000000                0.000000  1.064092e+05   \n",
       "75%                  0.000000                0.000000  1.187563e+05   \n",
       "max             681462.900000           397884.300000  2.203474e+07   \n",
       "\n",
       "             TARGET  \n",
       "count  76020.000000  \n",
       "mean       0.039569  \n",
       "std        0.194945  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 370 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = df.describe()\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_var2_0\n",
      "ind_var2\n",
      "ind_var27_0\n",
      "ind_var28_0\n",
      "ind_var28\n",
      "ind_var27\n",
      "ind_var41\n",
      "ind_var46_0\n",
      "ind_var46\n",
      "num_var27_0\n",
      "num_var28_0\n",
      "num_var28\n",
      "num_var27\n",
      "num_var41\n",
      "num_var46_0\n",
      "num_var46\n",
      "saldo_var28\n",
      "saldo_var27\n",
      "saldo_var41\n",
      "saldo_var46\n",
      "imp_amort_var18_hace3\n",
      "imp_amort_var34_hace3\n",
      "imp_reemb_var13_hace3\n",
      "imp_reemb_var33_hace3\n",
      "imp_trasp_var17_out_hace3\n",
      "imp_trasp_var33_out_hace3\n",
      "num_var2_0_ult1\n",
      "num_var2_ult1\n",
      "num_reemb_var13_hace3\n",
      "num_reemb_var33_hace3\n",
      "num_trasp_var17_out_hace3\n",
      "num_trasp_var33_out_hace3\n",
      "saldo_var2_ult1\n",
      "saldo_medio_var13_medio_hace3\n"
     ]
    }
   ],
   "source": [
    "description = df.describe()\n",
    "for column in df.columns:\n",
    "    if description[column]['min'] == description[column]['max']:\n",
    "        print(column)\n",
    "        del df[column]\n",
    "        del df_test[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var3\n"
     ]
    }
   ],
   "source": [
    "description = df.describe()\n",
    "for column in df.columns:\n",
    "    if description[column]['min'] <= -999999.000000:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column var3 has some really small numbers that don't make sense. Please those with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawn/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/shawn/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df.var3[df.var3 < 0] = 0\n",
    "df_test.var3[df_test.var3 < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a column contains 9999999999.0, then drop the column. These turn out to be the \"delta...\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "description = df.describe()\n",
    "large_num = 9999999999.0\n",
    "for column in df.columns:\n",
    "    if description[column]['max'] == large_num:\n",
    "        del df[column]\n",
    "        del df_test[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for column in df.columns[:-1]:\n",
    "#     max_ = description[column]['max']\n",
    "#     min_ = description[column]['min']\n",
    "#     df[column] = (df[column] - min_ )/ (max_ - min_)\n",
    "#     df_test[column] = (df_test[column] - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate_columns.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "duplicate_columns_filename = \"duplicate_columns.pickle\"\n",
    "duplicate_columns = set()\n",
    "if not os.path.exists(duplicate_columns_filename):\n",
    "    columns = df.columns\n",
    "    for i, column1 in enumerate(columns[:-1]):\n",
    "        for column2 in columns[i+1:]:\n",
    "            if df[column1].equals(df[column2]):\n",
    "                duplicate_columns.add(column2)\n",
    "    print(duplicate_columns)\n",
    "else:\n",
    "    print(\"Found \" + duplicate_columns_filename)\n",
    "    duplicate_columns = pickle.load( open( duplicate_columns_filename, \"rb\" ) )\n",
    "for c in duplicate_columns:\n",
    "    if c in df.columns:\n",
    "        del df[c]\n",
    "    if c in df_test.columns:\n",
    "        del df_test[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 76020 entries, 1 to 151838\n",
      "Columns: 273 entries, var3 to TARGET\n",
      "dtypes: float64(100), int64(173)\n",
      "memory usage: 158.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values between min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# description = df.drop(['TARGET'], axis=1).describe()\n",
    "# min_ = description['min':'min'].values.reshape(-1)\n",
    "# max_ = description['max':'max'].values.reshape(-1)\n",
    "# df.iloc[:,:-1] = (df.iloc[:,:-1] - min_) / (max_ - min_)\n",
    "# df_test = (df_test - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_all = df['TARGET']\n",
    "X_all = df.drop(['TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_all = scaler.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 138 ms, total: 2.73 s\n",
      "Wall time: 1.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79151117442538654"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = linear_model.LinearRegression()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 898 ms, sys: 24.6 ms, total: 922 ms\n",
      "Wall time: 928 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77633473956373333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = linear_model.LogisticRegression()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 526 ms, total: 1min 54s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83226159092397256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = GradientBoostingClassifier(max_depth=4, n_estimators=100) # 0.84267482954625106\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 127 ms, total: 18.7 s\n",
      "Wall time: 18.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8177783711321025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = AdaBoostClassifier()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 22.5 ms, total: 2.46 s\n",
      "Wall time: 2.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67162731621230565"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = RandomForestClassifier()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.63 s, sys: 51.4 ms, total: 4.68 s\n",
      "Wall time: 4.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63690892507218555"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = ExtraTreesClassifier()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "clf = svm.SVC()\n",
    "%time clf.fit(X_train, y_train)\n",
    "\n",
    "# pred = clf.predict_proba(X_test)\n",
    "pred = clf.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: None\n",
      "CPU times: user 2min 42s, sys: 2 s, total: 2min 44s\n",
      "Wall time: 2min 45s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ebb6ab2607de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print(\"caught keyboard interupt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tpot_pipeline.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tpot/tpot.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, testing_features, testing_classes)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimized_pipeline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A pipeline has not yet been optimized. Please call fit() first.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "from tpot import TPOT\n",
    "\n",
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size].values, test_size=0.20)\n",
    "\n",
    "tpot = TPOT(generations=5, verbosity=2)\n",
    "# try:\n",
    "%time tpot.fit(X_train, y_train)\n",
    "# except:\n",
    "#     print(\"caught keyboard interupt\")\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_pipeline.py')\n",
    "\n",
    "pred = tpot.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "\n",
    "def auc_score(pred, y_test):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test[:,0], pred[:,0])\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "batch_size = 256\n",
    "hidden_layer_size = [1024, 256]\n",
    "drop_out = np.array([0.0, 0.0]).astype(np.float64)\n",
    "beta = 1e-6\n",
    "initial_learning_rate = .6\n",
    "decay_steps = 500\n",
    "decay_rate = 0.98\n",
    "input_size = X_train.shape[1]\n",
    "num_steps = 20001 \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, input_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "  tf_test_dataset = tf.constant(X_test.values.astype(np.float32), tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([input_size, hidden_layer_size[0]], stddev=1.0/input_size, dtype=tf.float32))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_layer_size[0]], dtype=tf.float32))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size[0], hidden_layer_size[1]], stddev=tf.sqrt(1.0/hidden_layer_size[0]), dtype=tf.float32))\n",
    "#     tf.truncated_normal([hidden_layer_size[0], 1], stddev=tf.sqrt(1.0/hidden_layer_size[0]), dtype=tf.float32))\n",
    "  biases_2 = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size[1], 1], stddev=tf.sqrt(1.0/hidden_layer_size[1]), dtype=tf.float32))\n",
    "  biases_3 = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "  # Training computation.\n",
    "  def forward_prop(input_layer_1, training=False):\n",
    "    hidden_layer_2 = tf.nn.relu(tf.matmul(input_layer_1, weights_1) + biases_1)\n",
    "    if training and drop_out[0] > 0:\n",
    "        hidden_layer_2 = tf.nn.dropout(hidden_layer_2, drop_out[0])\n",
    "    output_layer_3 = tf.matmul(hidden_layer_2, weights_2) + biases_2\n",
    "#     if training and drop_out[1] > 0:\n",
    "#         hidden_layer_3 = tf.nn.dropout(hidden_layer_3, drop_out[1])\n",
    "#     output_layer_4 = tf.matmul(hidden_layer_3, weights_3) + biases_3\n",
    "    return output_layer_3\n",
    "  \n",
    "  logits = forward_prop(tf_train_dataset, training=True)\n",
    "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, tf_train_labels)) \\\n",
    "           + beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))# + tf.nn.l2_loss(weights_3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.sigmoid(logits)\n",
    "  test_prediction = tf.nn.sigmoid(forward_prop(tf_test_dataset))\n",
    "    \n",
    "def run_graph():\n",
    "    print(\"Beta: %f\" % beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (y_train.values.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = X_train.values[offset:(offset + batch_size), :]\n",
    "            batch_labels = y_train.values.reshape(-1, 1)[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data.astype(np.float32), tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 250 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch score: %.3f\" % auc_score(predictions, batch_labels))\n",
    "                print(\"Test score: %.3f\" % auc_score(test_prediction.eval(), y_test.values.reshape(-1, 1)))\n",
    "    \n",
    "%time run_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = len(X_all)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all[:sample_size], y_all[:sample_size], test_size=0.20)\n",
    "\n",
    "def auc_score(pred, y_test):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test[:,0], pred[:,0])\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "batch_size = 256\n",
    "hidden_layer_size = [1024, 256]\n",
    "drop_out = np.array([0.5, 0.5]).astype(np.float64)\n",
    "beta = 1e-7\n",
    "initial_learning_rate = .5\n",
    "decay_steps = 250\n",
    "decay_rate = 0.95\n",
    "input_size = X_train.shape[1]\n",
    "num_steps = 20001 \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, input_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "  tf_test_dataset = tf.constant(X_test.values.astype(np.float32), tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([input_size, hidden_layer_size[0]], stddev=1.0/input_size, dtype=tf.float32))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_layer_size[0]], dtype=tf.float32))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size[0], hidden_layer_size[1]], stddev=tf.sqrt(1.0/hidden_layer_size[0]), dtype=tf.float32))\n",
    "#     tf.truncated_normal([hidden_layer_size[0], 1], stddev=tf.sqrt(1.0/hidden_layer_size[0]), dtype=tf.float32))\n",
    "  biases_2 = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size[1], 1], stddev=tf.sqrt(1.0/hidden_layer_size[1]), dtype=tf.float32))\n",
    "  biases_3 = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "  # Training computation.\n",
    "  def forward_prop(input_layer_1, training=False):\n",
    "    hidden_layer_2 = tf.nn.relu(tf.matmul(input_layer_1, weights_1) + biases_1)\n",
    "    if training and drop_out[0] > 0:\n",
    "        hidden_layer_2 = tf.nn.dropout(hidden_layer_2, drop_out[0])\n",
    "    output_layer_3 = tf.matmul(hidden_layer_2, weights_2) + biases_2\n",
    "#     if training and drop_out[1] > 0:\n",
    "#         hidden_layer_3 = tf.nn.dropout(hidden_layer_3, drop_out[1])\n",
    "#     output_layer_4 = tf.matmul(hidden_layer_3, weights_3) + biases_3\n",
    "    return output_layer_3\n",
    "  \n",
    "  logits = forward_prop(tf_train_dataset, training=True)\n",
    "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, tf_train_labels)) \\\n",
    "           + beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))# + tf.nn.l2_loss(weights_3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.sigmoid(logits)\n",
    "  test_prediction = tf.nn.sigmoid(forward_prop(tf_test_dataset))\n",
    "    \n",
    "def run_graph():\n",
    "    print(\"Beta: %f\" % beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (y_train.values.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = X_train.values[offset:(offset + batch_size), :]\n",
    "            batch_labels = y_train.values.reshape(-1, 1)[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data.astype(np.float32), tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 250 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch score: %.3f\" % auc_score(predictions, batch_labels))\n",
    "                print(\"Test score: %.3f\" % auc_score(test_prediction.eval(), y_test.values.reshape(-1, 1)))\n",
    "    \n",
    "%time run_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
